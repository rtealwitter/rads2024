\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{amsmath, amsfonts}
\usepackage{hyperref}


\title{CSCI 1052 Problem Set 1}
\author{} % TODO: Put your name here
\date{\today}

\begin{document}

\maketitle

\subsection*{Submission Instructions}

Please upload your solutions by
\textbf{5pm Friday January 12, 2023.}
\begin{itemize}
\item You are encouraged to discuss ideas
and work with your classmates. However, you
\textbf{must acknowledge} your collaborators
at the top of each solution on which
you collaborated with others 
and you \textbf{must write} your solutions
independently.
\item Your solutions to theory questions must
be typeset in LaTeX or markdown.
I strongly recommend uploading the source LaTeX (found 
\href{https://www.rtealwitter.com/rads2024/files/pset1.tex}{here})
to Overleaf for editing.
\item I recommend that you write your solutions to coding question in a Jupyter notebook using Google Colab.
\item You should submit your solutions as a \textbf{single PDF} via the assignment on Canvas.
\end{itemize}

\subsection*{Problem 1 (from January 4)}
In class, we calculated the number of duplicates in a sample of size $m$ as
$$
D = \sum_{i=1}^m \sum_{j=i+1}^m D_{i,j}
$$
where $D_{i,j}$ is an indicator random variable that the $i$th and $j$th sampled items are the same.
In expectation when the samples are drawn uniformly at random, we saw that
$$
\mathbb{E}[D] = \frac{m (m-1)}{2 n}.
$$

\subsubsection*{Part 1} 
In practice, we know $m$ the number of samples we've taken and $D$ the number of duplicates in the sample.
Using these quantities, suggest a method for estimating $n$ the set size inspired by our expression for $\mathbb{E}[D]$.

\subsubsection*{Part 2}
Implement your method from Part 1 to estimate the number of unique articles in the English Wikipedia.
You can access ``random'' articles (see the discussion \href{https://en.wikipedia.org/wiki/Wikipedia:FAQ/Technical#random}{here}) by visiting the link: \url{https://en.wikipedia.org/wiki/Special:Random}.

According to the article \href{https://en.wikipedia.org/wiki/English_Wikipedia}{here}, English Wikipedia has 6.7 million articles. How does that compare to your estimate? How does the fact that the random article feature doesn't perfectly return random articles bias your estimate?

In Python, you can get a random URL by running the following code:

\begin{verbatim}
import requests
response = requests.get("https://en.wikipedia.org/wiki/Special:Random")
random_url = response.url	
\end{verbatim}

In my experiments, it took 30 minutes to get 5000 random articles. At this rate, it would take 28 days to get 6.7 million articles.
How long did your code take to run?

In your solution, include all relevant results and calculations in addition to a discussion of how accurate you think your estimate is.

%\input{solutions/solution1_1}


\end{document}